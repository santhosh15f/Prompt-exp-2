EXP 2: Comparative Analysis of Naïve Prompting versus Basic Prompting Using ChatGPT Across 
Various Test Scenarios Aim:
To test how ChatGPT responds to naïve prompts (broad or unstructured) versus basic 
prompts (clearer and more refined) across multiple scenarios, analyzing the quality, accuracy, and 
depth of the generated responses.

Scenario 1 : Testing Story Writing Skills Naïve
Prompt : “Write a short horror story.” Response :
![image](https://github.com/user-attachments/assets/23c518ee-0e9f-444b-84df-5fb5b3c9fdaa)
Basic Prompt : “Write a short story about an explorer who is lost in the Amazon forest due to 
aeroplane crash, the mood should be 2000’s kind of story in John R. Leonetti style. Follow horror 
genre. Restrict the story with 150 words” Response :
![image](https://github.com/user-attachments/assets/84659f88-47bd-49c0-a014-46438a057e73)

Scenario 2 : Answering Factual Research based Questions Naïve 
Prompting : “What is Acid rain?” Response:
![image](https://github.com/user-attachments/assets/7d898075-61fb-449d-99bf-a532a1e055fd)
Basic Promp ng : “Explain about climate change, including it’s causes and effects on the 
environment. List the responsibil es as we humans should take to prevent extreme effects of clima c 
change.
![image](https://github.com/user-attachments/assets/413bf215-5bf9-4c6f-8656-b0416047ebbd)

Scenario 3 : Coding Based Ques ons
Naïve Promp ng : “Provide a code for finding Armstrong number.” Response
![image](https://github.com/user-attachments/assets/f535cc56-b6f8-4e8b-95f6-2c8bbbed8331)
Basic Promp ng : “Can you give me a Python language code for finding Armstrong number? Give 
me the most op mal algorithms to sort with minimal me complexity and space complexity.” 
Response :
![image](https://github.com/user-attachments/assets/2bf65ee3-fd66-411d-8ede-4084355ffd81)
Here's the structured report comparing ChatGPT's responses to the naive and basic prompts from the 
specified scenarios:
Comparison of ChatGPT's Responses to Naïve vs. Basic Prompts
Scenario Prompt Type Example Prompt Response Quality Accuracy Depth Notes

Table Comparing ChatGPT’s Responses to Naïve and Basic Prompts Across Scenarios
Naïve Prompt "Write a short horror story." Basic Prompt "Write a short story about an explorer who 
is lost in the Amazon forest due to an aeroplane crash..."
![WhatsApp Image 2024-11-20 at 17 28 40_16071c59](https://github.com/user-attachments/assets/5aab408d-8c21-43b2-a72f-ae925a2d4b29)
Naïve Prompt: "What is Acid rain?" Basic Prompt: "Explain about climate change, including its 
causes and effects on the environment. List the responsibili es we humans should take to prevent 
extreme effects of climate change.
![WhatsApp Image 2024-11-20 at 17 28 39_feacd0bd](https://github.com/user-attachments/assets/ed0bcf0e-a281-46aa-b7b6-b0d166d18c32)
![WhatsApp Image 2024-11-20 at 17 28 39_b7406684](https://github.com/user-attachments/assets/0ea9ace0-2585-4b25-8fd5-0ed95b118efd)
![WhatsApp Image 2024-11-20 at 17 28 34_0c0d425b](https://github.com/user-attachments/assets/215f8e01-47b3-47f1-b871-3898fcf5c7f6)
Analysis of Prompt Clarity Impac ng Output Quality 
Quality:
1. Naïve Prompts: Responses to vague prompts, such as “Write a horror story,” o en lack direc 
on and depth. The model may generate a basic narra ve without a defined theme or structure. 
Similarly, “What is acid rain?” and “Can you give me a code for finding armstrong number.” 
yield general or incomplete responses due to their lack of specificity.
2. Basic Prompts: Clear and detailed prompts like “Write a short story about an explorer who is 
lost in the Amazon forest due to an aeroplane crash...” and “Explain about climate change, 
including its causes and effects on the environment…” lead to higher-quality responses. The 
model can leverage the addi onal context to produce more informa ve and engaging outputs.
Accuracy:
3. Naïve Prompts: Prompts like “What is acid rain?” may result in moderate accuracy, 
providing a general overview but lacking the depth necessary for a complete understanding 
of the topic. The coding prompt does not specify the programming language or requirements, 
which may lead to ambiguity in the response.
4. Basic Prompts: More focused prompts, such as “Provide a code for finding Armstrong 
number” ensure higher accuracy in responses. They help the model hone in on specific 
informa on, leading to be er-informed answers.
Depth:
5. Naïve Prompts: Responses to open-ended or vague prompts tend to be shallow, as seen in the 
naive responses to storytelling and coding ques ons. The AI may provide only surfacelevel 
informa on without exploring cri cal details or nuances.
6. Basic Prompts: By specifying requirements and context, such as mood, genre, and coding 
language, users elicit richer, more detailed responses. For instance, the request for a story 

about an astronaut not only defines the narra ve but also sets a specific mood and style, 
enhancing depth.
Conclusion
The quality, precision, and depth of ChatGPT's outputs in a variety of contexts are strongly impacted 
by the prompts' clarity. While fundamental prompts that give clear instruc ons produce high-quality, 
accurate, and detailed informa on, naïve prompts frequently result in responses that are ambiguous, 
generic, or superficial. Users can improve their interac ons with ChatGPT and increase the efficacy 
of its responses by crea ng clear prompts that specify specific needs. This analysis emphasizes how
crucial prompt engineering is to making efficient use of AI models like ChatGPT, emphasizing that 
precision and lucidity are necessary to get peak performance.
